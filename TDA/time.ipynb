{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMsIJ7wzt-1M",
    "outputId": "31585b45-338a-4cab-dc57-ba9aef2190dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import Delaunay\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.animation as ani\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulation(file_name):\n",
    "    df = pd.read_csv(file_name, sep=\"\\s+\",header = None,usecols=[1,2])\n",
    "    points = df[[1,2]].to_numpy()\n",
    "    tri = Delaunay(points)\n",
    "    _neighbors = defaultdict(set)\n",
    "    for simplex in tri.vertices:\n",
    "        for i, j in permutations(simplex, 2):\n",
    "            _neighbors[i].add(j)\n",
    "    #list those neighbors via their index\n",
    "    points = [tuple(p) for p in tri.points]\n",
    "    neighbors = {}\n",
    "    for k, v in _neighbors.items():\n",
    "        neighbors[k] = [y for y in v]\n",
    "    G = nx.Graph()\n",
    "    for p in range(len(points)):\n",
    "        G.add_node(p)\n",
    "    pos = {}\n",
    "    for e in neighbors:\n",
    "        d = points[e]\n",
    "        pos[e] = [s for s in d]    \n",
    "    G.remove_edges_from(G.edges())\n",
    "    for h in neighbors:\n",
    "        node1 = h\n",
    "        for g in neighbors[h]:\n",
    "            node2 = g\n",
    "            if distance.euclidean(points[node1],points[node2]) < 4:\n",
    "                G.add_edge(node1,node2)\n",
    "                \n",
    "    return G, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time plots for TDA Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "velo = [0.01, 0.1]\n",
    "nois = [1.0, 6.0]\n",
    "neig = [2]\n",
    "gapp = [1.0, 0.5, 0.0]\n",
    "ang = [12, 2]\n",
    "repli = range(0, 16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for vel in velo:\n",
    "    for noi in nois:\n",
    "        for nei in neig:\n",
    "            for ga in gapp:\n",
    "                for angl in ang:\n",
    "                    for k in repli:\n",
    "                        dir_path = '/Users/surajsahu/Documents/Vasculogensis/Simulation/TDA/simulation/codes/'\n",
    "                        os.chdir(dir_path + 'velocity_'+str(vel)+'noise_'+str(noi)+'gap_'+str(ga)+'neigh_'+str(nei)+'angle_'+str(angl)+ '/'+str(k)+'/video/frames/')\n",
    "                        files = sorted(os.listdir())\n",
    "                        avg_neigh_degree = []\n",
    "                        density=[]\n",
    "                        avg_clustering = []\n",
    "                        degree_centrality = []\n",
    "                        closeness_centrality = []\n",
    "                        bet_centrality = []\n",
    "                        for j in range(0,200,1):\n",
    "                            G, pos = triangulation(files[j])\n",
    "                            avg_neigh_degree.append(np.array(list(nx.average_neighbor_degree(G).values())).mean())\n",
    "                            density.append(nx.density(G))\n",
    "                            avg_clustering.append(nx.average_clustering(G))\n",
    "                            degree_centrality.append(np.array(list(nx.degree_centrality(G).values())).mean())\n",
    "                            closeness_centrality.append(np.array(list(nx.closeness_centrality(G).values())).mean())\n",
    "                            bet_centrality.append(np.array(list(nx.betweenness_centrality(G).values())).mean())\n",
    "                        dict = {'avg_neigh_degree': avg_neigh_degree, 'density': density, 'avg_clustering': avg_clustering, 'degree_centrality': degree_centrality, 'closeness_centrality': closeness_centrality, 'bet_centrality': bet_centrality}\n",
    "                        df = pd.DataFrame(dict)\n",
    "                        os.chdir(dir_path+'velocity_'+str(vel)+'noise_'+str(noi)+'gap_'+str(ga)+'neigh_'+str(nei)+'angle_'+str(angl)+ '/'+str(k)+'/data/')\n",
    "                        df.to_csv('tda.csv')\n",
    "                        print('done')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for vel in velo:\n",
    "    for noi in nois:\n",
    "        for nei in neig:\n",
    "            for ga in gapp:\n",
    "                for angl in gapp:\n",
    "                    df = []\n",
    "                    for k in repli:\n",
    "                        os.chdir(dir_path+'velocity_'+str(vel)+'noise_'+str(noi)+'gap_'+str(ga)+'neigh_'+str(nei)+'angle_'+str(angl)+ '/'+str(k)+'/data/')\n",
    "                        df.append(pd.read_csv('tda.csv'))\n",
    "                    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/surajsahu/Documents/Vasculogensis/Simulation/TDA/simulation/codes/'\n",
    "velo = [0.01]\n",
    "nois = [1.0]\n",
    "neig = [2]\n",
    "gapp = [1.0, 0.5, 0.0]\n",
    "ang = [2]\n",
    "repli = range(0, 16, 1)\n",
    "for vel in velo:\n",
    "    for noi in nois:\n",
    "        for nei in neig:\n",
    "            for ga in gapp:\n",
    "                for angl in ang:\n",
    "                    df = []\n",
    "                    for k in repli:\n",
    "                        os.chdir(dir_path+'velocity_'+str(vel)+'noise_'+str(noi)+'gap_'+str(ga)+'neigh_'+str(nei)+'angle_'+str(angl)+ '/'+str(k)+'/data/')\n",
    "                        file_name = 'tda.csv'\n",
    "                        df.append(pd.read_csv(file_name))\n",
    "                    a = []\n",
    "                    for i in repli:\n",
    "                        file_name1 = 'tda.csv'\n",
    "                        a.append(list(df[i]['avg_neigh_degree']))\n",
    "                    a = [sum(col) / float(len(col)) for col in zip(*a)]\n",
    "                    plt.plot(a)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/surajsahu/Documents/Vasculogensis/Simulation/TDA/simulation/codes/'\n",
    "velo = [0.01, 0.03]\n",
    "gapp = [1.0]\n",
    "nois = [1.0]\n",
    "neig = [2]\n",
    "ang = [2]\n",
    "repli = [0, 1, 2, 3, 4, 5]\n",
    "for vel in velo:\n",
    "    for noi in nois:\n",
    "        for nei in neig:\n",
    "            for ga in gapp:\n",
    "                for angl in ang:\n",
    "                    df = []\n",
    "                    for k in repli:\n",
    "                        os.chdir(dir_path+'velocity_'+str(vel)+'noise_'+str(noi)+'gap_'+str(ga)+'neigh_'+str(nei)+'angle_'+str(angl)+ '/'+str(k)+'/data/')\n",
    "                        file_name = 'tda.csv'\n",
    "                        df.append(pd.read_csv(file_name))\n",
    "                    a = []\n",
    "                    for i in repli:\n",
    "                        file_name1 = 'tda.csv'\n",
    "                        a.append(list(df[i]['avg_neigh_degree']))\n",
    "                    a = [sum(col) / float(len(col)) for col in zip(*a)]\n",
    "                    plt.plot(a)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "os.chdir(dir_path+ '/velocity_0.01noise_1.0gap_1.0neigh_2angle_2/0/data/')\n",
    "file_name1 = 'tda.csv'\n",
    "df.append(pd.read_csv(file_name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir_path+ '/velocity_0.01noise_1.0gap_1.0neigh_2angle_2/1/data/')\n",
    "file_name2 = 'tda.csv'\n",
    "df.append(pd.read_csv(file_name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = list(df[0]['avg_neigh_degree'])\n",
    "d2 = list(df[1]['avg_neigh_degree'])\n",
    "plt.plot(d1)\n",
    "plt.plot(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + filename, sep=\"\\s+\",header = None,usecols=[0,1])\n",
    "points = df[[0,1]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "d=[]\n",
    "e=[]\n",
    "f=[]\n",
    "for k in range(0,10,1): \n",
    "    files=[]\n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"codes/velocity_0.01noise_1.0gap_\"+str(1.0)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"):\n",
    "        files.extend([\"codes/velocity_0.01noise_1.0gap_\"+str(1.0)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"+filenamest for filenamest in filenames if \"DS_Store\" not in filenamest])\n",
    "        break\n",
    "    files = sorted(files)\n",
    "    edges=[]\n",
    "    avg_neigh_degree = []\n",
    "    density=[]\n",
    "    avg_clustering = []\n",
    "    degree_centrality = []\n",
    "    closeness_centrality = []\n",
    "    bet_centrality = []\n",
    "    for j in range(0,200,1):\n",
    "        file_name = files[j]\n",
    "        G, pos = triangulation(file_name)\n",
    "        avg_neigh_degree.append(np.array(list(nx.average_neighbor_degree(G).values())).mean())\n",
    "        density.append(nx.density(G))\n",
    "        avg_clustering.append(nx.average_clustering(G))\n",
    "        degree_centrality.append(np.array(list(nx.degree_centrality(G).values())).mean())\n",
    "        closeness_centrality.append(np.array(list(nx.closeness_centrality(G).values())).mean())\n",
    "        bet_centrality.append(np.array(list(nx.betweenness_centrality(G).values())).mean())\n",
    "    a.append(avg_neigh_degree)\n",
    "    b.append(density)\n",
    "    c.append(avg_clustering)\n",
    "    d.append(degree_centrality)\n",
    "    e.append(closeness_centrality)\n",
    "    f.append(bet_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20,6),constrained_layout=True)\n",
    "fig.suptitle('TDA metrics with time', fontsize=16)\n",
    "for i in a:\n",
    "    axs[0].plot(i)\n",
    "axs[0].set_title('Average Neighbour Degree with time')\n",
    "axs[0].set_xlabel('Time steps')\n",
    "axs[0].set_ylabel('Average Neighbour Degree')\n",
    "\n",
    "    \n",
    "for i in b:\n",
    "    axs[1].plot(i)\n",
    "axs[1].set_title('Density with time')\n",
    "axs[1].set_xlabel('Time steps')\n",
    "axs[1].set_ylabel('Density')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,6),constrained_layout=True)\n",
    "for i in c:\n",
    "    axs[0].plot(i)\n",
    "axs[0].set_title('Average Clustering Coeff. with Time')\n",
    "axs[0].set_xlabel('Time steps')\n",
    "axs[0].set_ylabel('Average Clustering Coeff.')\n",
    "    \n",
    "for i in d:\n",
    "    axs[1].plot(i)\n",
    "axs[1].set_title('Degree Centrality with time')\n",
    "axs[1].set_xlabel('Time steps')\n",
    "axs[1].set_ylabel('Degree Centrality')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,6),constrained_layout=True)\n",
    "for i in e:\n",
    "    axs[0].plot(i)\n",
    "axs[0].set_title('Closeness Centrality with time')\n",
    "axs[0].set_xlabel('Time steps')\n",
    "axs[0].set_ylabel('Closeness Centrality')\n",
    "    \n",
    "for i in f:\n",
    "    axs[1].plot(i)\n",
    "axs[1].set_title('Betweeness Centrality with time')\n",
    "axs[1].set_xlabel('Time steps')\n",
    "axs[1].set_ylabel('Betweeness Centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Average Neighbour Degree with time for different Inturption Factor')\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Average Neighbour Degree')\n",
    "for i in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    a=[]\n",
    "    for k in range(0,20,1): \n",
    "        files=[]\n",
    "        for (dirpath, dirnames, filenames) in os.walk(\"codes/velocity_0.01noise_1.0gap_\"+str(i)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"):\n",
    "            files.extend([\"codes/velocity_0.01noise_1.0gap_\"+str(i)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"+filenamest for filenamest in filenames if \"DS_Store\" not in filenamest])\n",
    "            break\n",
    "        files = sorted(files)\n",
    "        avg_neigh_degree = []\n",
    "        for j in range(0,200,1):\n",
    "            file_name = files[j]\n",
    "            G, pos = triangulation(file_name)\n",
    "            avg_neigh_degree.append(np.array(list(nx.average_neighbor_degree(G).values())).mean())\n",
    "        a.append(avg_neigh_degree)\n",
    "    p = [sum(x) for x in zip(*a)]\n",
    "    plt.plot(p, label = i)\n",
    "    plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('closeness_centrality with time for different Inturption Factor')\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Acloseness_centrality')\n",
    "for i in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    a=[]\n",
    "    for k in range(0,10,1): \n",
    "        files=[]\n",
    "        for (dirpath, dirnames, filenames) in os.walk(\"codes/velocity_0.01noise_1.0gap_\"+str(i)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"):\n",
    "            files.extend([\"codes/velocity_0.01noise_1.0gap_\"+str(i)+\"neigh_2.0angle_2/\"+str(k)+\"/video/frames/\"+filenamest for filenamest in filenames if \"DS_Store\" not in filenamest])\n",
    "            break\n",
    "        files = sorted(files)\n",
    "        closeness_centrality = []\n",
    "        for j in range(0,1999,1):\n",
    "            file_name = files[j]\n",
    "            G, pos = triangulation(file_name)\n",
    "            closeness_centrality.append(np.array(list(nx.closeness_centrality(G).values())).mean())\n",
    "        a.append(closeness_centrality)\n",
    "    p = [sum(x) for x in zip(*a)]\n",
    "    plt.plot(p, label = i)\n",
    "    plt.legend()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VascularDevelopmentTDANotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
